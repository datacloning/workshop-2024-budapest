---
title: "Data Cloning Workshop, Budapest 2024"
author: "Subhash Lele and Peter Solymos"
date: "2024-08-28"
output: pdf_document
---

```{r include=FALSE}
set.seed(0)
```

## Preliminaries

In the first part of the course, we will go over some statistical preliminaries and corresponding computational aspects. We will learn:

1. To write down a likelihood function
2. Meaning of the likelihood function
3. Meaning of the Maximum likelihood estimator, difference between a parameter, an estimator and an estimate
4. Good properties of an estimator: Consistency, Asymptotic normality, Unbiasedness, low Mean squared error
5. Frequentist paradigm and quantification of uncertainty
6. How to use Fisher information for an approximate quantification of uncertainty
7. Motivation for the Bayesian paradigm
8. Meaning of the prior distribution
9. Derivation and meaning of the posterior distribution
10. Interpretation of a credible interval and a confidence interval
11. Scope of inference: Should I specify the hypothetical experiment or should I specify the prior distribution? Each one comes with its own scope of inference.

### Occupancy studies

Let us start with an occupancy study. Suppose we have a site that is being considered for development. There is a species of interest that might get affected by this development. Hence we need to study what proportion of the area is occupied by the species of interest. If this proportion is not very large, we may go ahead with the development. 

Suppose we divide the site in several equal-area cells. Suppose all cells have similar habitats (_identical_). Further we assume that occupancy of one cell does not affect occupancy of other quadrats (_independence_). Let $N$ be the total number of cells.

Let $Y_i$ be the occupancy status of the _i_-th quadrat. This is unknown and hence is a random variable. It takes values in (0,1), 0 meaning unoccupied and 1 meaning occupied. This is a _Bernoulli_ random variable. We denote this by $Y_{i}\sim Bernoulli(\phi)$. The random variable $Y$ takes value 1 with probability $\phi$. This is the probability of occupancy. The value of $\phi$ is unknown. This is the parameter of the distribution. 

Suppose we visit $n$, a subset, of these cells. These are selected using simple random sampling without replacement. The observations are denoted by $y_1,y_2,y_3,...,y_n$. We can use these to infer about the unknown parameter $\phi$. The main tool for such inductive inference (data to population and _not_ hypothesis to prediction) is the _likelihood function_.

#### The likelihood function

Suppose the observed data are (0,1,1,0,0). Then we can compute the probability of observing these data under various values of the parameter $\phi$ (assuming independent, identically distributed Bernoulli random variables). It can be written as:
$L(\phi;y_{1},y_{2},...,y_{n})={\prod}P(y_{i};\phi)={\prod}\phi^{y_{i}}(1-\phi)^{1-y_{i}}$

Notice that this is a function of the parameter $\phi$ and the data are fixed. The likelihood function or equivalently the log-likelihood function quantifies the _relative_ support for different values of the parameters. Hence only the likelihood ratio function is meaningful. 

#### Maximum likelihood estimator

A natural approach to estimation (inference) of $\phi$ is to choose the value that is better supported than any other value in the parameter space $(0,1)$. This is called the maximum likelihood estimator. We can show that this turns out to be:
$\hat{\phi}=\frac{1}{n}\sum y_{i}$
This is called an _estimate_. This is a fixed quantity because the data are observed and hence not random. 

#### Quantification of uncertainty

As scientists, would you stop at reporting this? I suspect not. If this estimate is large, say 0.85, the developer is going to say 'you just got lucky (or, worse, you cheated) with your particular sample'. A natural question to ask then is 'how different this estimate would have been if someone else had conducted the experiment?'. In this case, the 'experiment to be repeated' is fairly uncontroversial. We take another simple random sample without replacement from the study area. However, that is not always the case as we will see when we deal with the regression model. 

The sampling distribution is the distribution of the estimates that one would have obtained had one conducted these replicate experiments. It is possible to get an approximation to this sampling distribution in a very general fashion if we use the method of maximum likelihood estimator. In many situations, it can be shown that the sampling distribution is 
$\hat{\phi}\sim N(\phi,\frac{1}{n}I^{-1}(\phi))$
where 
$I(\phi)=-\frac{1}{n}\sum\frac{d^2}{d^2\phi}logL(\phi;{y})$

This is also called the Hessian matrix or the curvature matrix of the log-likelihood function. The higher the curvature, the less variable are the estimates from one experiment to other. Hence the resultant 'estimate' is considered highly reliable. 

#### 95% Confidence interval

This is just a set of values that covers the estimates from 95% of the experiments. The experiments are not actually replicated and hence this simply tells us what the various outcomes could be. Our decisions could be based on this variation _as long as we all agree on the experiment that could be replicated_. We are simply covering our bases against the various outcomes and protect ourselves from future challenges. If we use the maximum likelihood estimator, we can obtain this as:
$\hat{\phi}-\frac{1.96}{n}\sqrt{I^{-1}(\hat{\phi})},\hat{\phi}+\frac{1.96}{n}\sqrt{I^{-1}(\hat{\phi})}$
You will notice that as we increase the sample size, the width of this interval converges to zero. That is, as we increase the sample size, the MLE converges to the true parameter value. This is called the 'consistency' of an estimator. This is an essential property of any statistical inferential procedure.

### Bayesian paradigm

All the above statements seem logical but fake at the same time! No one repeats the same experiment (although replication consistency is an essential scientific requirement). What if we have time series? We can never replicate a time series. So then should we simply take the estimated value _prima facie_? That also seems incorrect scientifically. So where is the uncertainty in our mind coming from? According to the Bayesian paradigm, it arises because of our 'personal' uncertainty about the parameter values. 

**Prior distribution**: Suppose we have some idea about what values of occupancy are more likely than others _before_ any data are collected. This can be quantified as a probability distribution on the parameter space (0,1). This distribution can be anything, unimodal or bimodal or even multimodal! Let us denote this by $\pi(\phi)$. How do we change this _after_ we observe the data? 

**Posterior distribution** This is the quantification of uncertainty _after_ we observe the data. Usually observing the data decreases our uncertainty, although it is not guaranteed to be the case. The posterior distribution is obtained by:
$\pi(\phi|y)=\frac{L(\phi;y)\pi(\phi)}{\int L(\phi;d\phi y)\pi(\phi)}$


#### Credible interval

This is obtained by using the percentiles of the posterior distribution. 

Notice a few things here:

1. This involves an integral in the denominator. Depending on how many parameters (unknowns) are in the model, this can be a large dimensional integral. Imagine a regression model with 5 covariates. This integral will be 6 dimensional (add one for the variance).
2. Data are fixed. We do not need to replicate the experiment. The uncertainty is completely in the mind of the researcher.
3. Different researchers might have different prior uncertainties. This will lead to different posterior uncertainties. Hence this is a subjective or personal quantification of uncertainty. It is not transferable from one researcher to another.

An interesting result follows. As we increase the samples size, the Bayesian posterior, for ANY prior, converges to the distribution that looks very much like the frequentist sampling distribution of the MLE. That is,
$\pi(\phi|y)\thickapprox N(\hat{\phi},\frac{1}{n}I^{-1}(\hat{\phi}))$
There are subtle differences that we are going to ignore here. Qualitatively, what this says is that for large sample size:

1. Posterior mean and the MLE are similar 
2. Posterior variance is similar to the inverse of the Hessian matrix. 

Hence credible interval and confidence intervals will be indistinguishable for large sample size. Effect of the choice of the prior distribution vanishes. How large a sample size should be for this to happen? It depends on the number of parameters in the model and how strong the prior distribution is. 

### No math please!

> Bayesian and ML inference using MCMC and data cloning

We now show how one can compute the posterior distribution for any choice of the prior distribution without analytically calculating the integral in the denominator. We will generate the data under the Bernoulli model. You can change the parameters as you wish when you run the code.

Simulate a simple data set with 30 observations:

```{r}
library(dclone)

phi.true = 0.3
n = 30
Y = rbinom(n,1,phi.true)

table(Y)
```

Analytical MLE:

```{r}
(MLE.est = sum(Y)/n)
```

#### Bayesian inference

We will use the [JAGS](https://mcmc-jags.sourceforge.io/) program and the [dclone](https://CRAN.R-project.org/package=dclone) R package.

First, we need to define the model function. This is the critical component. 

```{r}
Occ.model = function(){
  # Likelihood 
  for (i in 1:n){
    Y[i] ~ dbin(phi_occ, 1)
  }
  # Prior
  phi_occ ~ dbeta(1, 1)
}
```

Second, we need to provide the data to the model and generate random numbers from the posterior. We will discuss different options later. 

```{r}
dat = list(Y=Y, n=n)
Occ.Bayes = jags.fit(data=dat, params="phi_occ", model=Occ.model)
summary(Occ.Bayes)
plot(Occ.Bayes)
```

The summary describes the posterior distribution: its mean, standard deviation, and quantiles.

This was quite easy. Now we use data cloning to compute the MLE and its variance using MCMC.

### Data cloning in a nutshell

As you all know, at least in this simple situation, we can write down the likelihood function analytically. We can also use calculus and/or numerical optimization such as the `optim()` function in R to get the location of the maximum and its Hessian matrix. But suppose we do not want to go through all of that and instead want to use the MCMC algorithm. Why? Because it is easy and can be generalized to more complex hierarchical models.

Earlier we noted that as we increase the sample size, the Bayesian posterior converges to the sampling distribution of the MLE. We, obviously, cannot increase the sample size. The data are given to us. Data cloning conducts a computational trick to increase the sample size. We clone the data!

Imagine a sequence of _K_ independent researchers. 

- Step 1: First researcher has data $y_1,y_2,...,y_n$. They use their own prior and obtain the posterior distribution.
- Step 2: Second researcher goes out and gets their own data. It just so happens that they observed the same exact locations as the first researcher. Being a good Bayesian, they use the posterior of the first researcher as their prior (knowledge accumulation).
- Step K: The K-th researcher also obtains the same data but uses the posterior at the (K-1) step as their prior.

What is happening with these sequential posterior distributions?

**The posterior distribution is converging to a single point; a degenerate distribution. This is identical to the MLE!**

1. As we increase the number of clones, the mean of the posterior distributions converges to the MLE.
2. The variance of the posterior distribution converges to 0.
3. If we scale the posterior distribution with the number of clones (that is, multiply the posterior variance by the number of clones), it is identical to the inverse of the Fisher information matrix. 

You can play with the number of clones and see the effect on the posterior distribution using this [Shiny app](https://psolymos.shinyapps.io/dcapps/) ([R code for the app](../app/))

We do not need to implement this procedure sequentially. The matrix  of these K datasets is of dimension (_n_,_K_) with identical columns. 

$\left[\begin{array}{cccccccccc}
y_{1} & y_{1} & y_{1} & y_{1} & y_{1} & y_{1} & y_{1} & y_{1} & y_{1} & y_{1}\\
y_{2} & y_{2} & y_{2} & y_{2} & y_{2} & y_{2} & y_{2} & y_{2} & y_{2} & y_{2}\\
y_{3} & y_{3} & y_{3} & y_{3} & y_{3} & y_{3} & y_{3} & y_{3} & y_{3} & y_{3}\\
y_{4} & y_{4} & y_{4} & y_{4} & y_{4} & y_{4} & y_{4} & y_{4} & y_{4} & y_{4}\\
y_{5} & y_{5} & y_{5} & y_{5} & y_{5} & y_{5} & y_{5} & y_{5} & y_{5} & y_{5}
\end{array}\right]$

We use the Bayesian procedure to analyze these data. The model function used previously can be used with a minor modification to do this. 

```{r}
Occ.model.dc = function(){
  # Likelihood 
  for(k in 1:ncl){
    for (i in 1:n){
      Y[i,k] ~ dbin(phi_occ, 1)
    }
  }
  # Prior
  phi_occ ~ dbeta(1, 1)
}
```

To match this change in the model, we need to turn the original data into an array.

```{r}
Y = array(Y, dim=c(n, 1))
Y = dcdim(Y)
```

When defining the data, we need to add another index `ncl` for the cloned dimension. It gets multiplied by the number of clones.

```{r}
dat = list(Y=Y, n=n, ncl=1)
# 2 clones of the Y array
dclone(Y, 2)
# 2 clones of the data list - this is not what we want
dclone(dat, 2)
```

Notice that this changes `n` also. We do not want that, we want to keep `n` unchanged.

```{r}
dclone(dat, 2, unchanged="n", multiply="ncl")
```

The `dc.fit` function takes the familiar arguments to determine how to clone the data list.

```{r}
Occ.DC = dc.fit(data=dat, params="phi_occ", model=Occ.model.dc,
  n.clones=c(1, 2, 5), unchanged="n", multiply="ncl")
summary(Occ.DC)
plot(Occ.DC)
```

Notice the `Mean`, `DC SD`, and `R hat` columns in the summary. These refer to the maximum likelihood estimate, the asymptotic standatd error (SD of the posterior times square root of _K_), and the Gelman-Rubin disgnostic:

```{r}
coef(Occ.DC) # MLE
dcsd(Occ.DC) # SE=SD*sqrt(K)
gelman.diag(Occ.DC) # R hat
```

#### Summaries for the clones

Summaries of the posterior distributions for the different numbers of clones are saved and we can print these out with the `dctable()` command. We can visualize these with the `plot` function.

```{r}
dctable(Occ.DC)
dctable(Occ.DC) |> plot()
```

Some data cloning related diagnostics are printed with the `dcdiag()` function. We will discuss these statistics in detail. The most important thing to ckeck is that the solid line follows the scattered line for `lambda.max`, i.e. decreases with the number of clones.

```{r}
dcdiag(Occ.DC)
dcdiag(Occ.DC) |> plot()
```

### Regression models

Now we will generalize these models to account for covariates. We will consider Logistic regression but also comment on how to change it to Probit regression easily. Similarly we show how this basic prototype can be modified to do linear and non-linear regression, Poisson regression etc. 

```{r}
n = 30 # sample size
X1 = rnorm(n) # a covariate
X = model.matrix(~X1)
beta.true = c(0.5, 1)
link_mu = X %*% beta.true # logit scale
```

#### Logistic regression model

```{r}
phi_occ = plogis(link_mu) # prob scale
Y = rbinom(n, 1, phi_occ)
```

Maximum likelihood estimate using `glm()`:

```{r}
MLE.est = glm(Y ~ X1, family="binomial")
```

Bayesian analysis

```{r}
Occ.model = function(){
  # Likelihood 
  for (i in 1:n){
    phi_occ[i] <- ilogit(X[i,] %*% beta)
    Y[i] ~ dbin(phi_occ[i], 1)
  }
  # Prior
  beta[1] ~ dnorm(0, 1)
  beta[2] ~ dnorm(0, 1)
}
```

Now we need to provide the data to the model and generate random numbers from the posterior. We will discuss different options later. 

```{r}
dat = list(Y=Y, X=X, n=n)
Occ.Bayes = jags.fit(data=dat, params="beta", model=Occ.model)
summary(Occ.Bayes)
plot(Occ.Bayes)
pairs(Occ.Bayes)
```

Now we modify this to get the MLE using data cloning.

```{r}
Occ.model_dc = function(){
  # Likelihood 
  for (k in 1:ncl){
    for (i in 1:n){
      phi_occ[i,k] <- ilogit(X[i,,k] %*% beta)
      Y[i,k] ~ dbin(phi_occ[i,k],1)
    }
  }
  # Prior
  beta[1] ~ dnorm(0, 1)
  beta[2] ~ dnorm(0, 1)
}
```

Now we need to provide the data to the model and generate random numbers from the posterior. We will discuss different options later. 

```{r}
Y = array(Y, dim=c(n, 1))
X = array(X, dim=c(dim(X), 1))
# clone the objects
Y = dcdim(Y)
X = dcdim(X)
```

Data cloning with `dc.fit()`:

```{r}
dat = list(Y=Y, X=X, n=n, ncl=1)
Occ.DC = dc.fit(data=dat, params="beta", model=Occ.model_dc,
  n.clones=c(1, 2, 5), unchanged="n", multiply="ncl")
summary(Occ.DC)
plot(Occ.DC)
pairs(Occ.DC)
```

These are the familiar functions for the asymptotic ML inference (already accounted for the number of clones):

```{r}
coef(Occ.DC) # MLE
dcsd(Occ.DC) # SE
vcov(Occ.DC) # asymptotic VCV
confint(Occ.DC, level=0.95) # 95% CI
```

Let's check the posterior summaries and the DC diagnostics:

```{r}
dctable(Occ.DC)
dctable(Occ.DC) |> plot()
dcdiag(Occ.DC)
dcdiag(Occ.DC) |> plot()
```

We hope you can see the pattern in how we are changing the prototype model function and the data function. If we want to do a Normal linear regression and Poisson regression we can modify the regression program above easily.

#### Linear regression

The following section issustrates Gaussian linear regression.

```{r}
n = 30
X1 = rnorm(n)
X = model.matrix(~X1)
beta.true = c(0.5, 1)
link_mu = X %*% beta.true

# Linear regression model
mu = link_mu
sigma.e = 1
Y = rnorm(n,mean=mu,sd=sigma.e)

# MLE
MLE.est = glm(Y ~ X1, family="gaussian")

# Bayesian analysis
Normal.model = function(){
  # Likelihood 
  for (i in 1:n){
    mu[i] <- X[i,] %*% beta
    Y[i] ~ dnorm(mu[i],prec.e)
  }
  # Prior
  beta[1] ~ dnorm(0, 1)
  beta[2] ~ dnorm(0, 1)
  prec.e ~ dlnorm(0, 1)
}
dat = list(Y=Y, X=X, n=n)
Normal.Bayes = jags.fit(data=dat, params=c("beta","prec.e"), model=Normal.model)
summary(Normal.Bayes)
plot(Normal.Bayes)
pairs(Normal.Bayes)

# MLE using data cloning.
Normal.model_dc = function(){
  # Likelihood 
  for (k in 1:ncl){
    for (i in 1:n){
      mu[i,k] <- X[i,,k] %*% beta
      Y[i,k] ~ dnorm(mu[i,k],prec.e)
    }
  }
  # Prior
  beta[1] ~ dnorm(0, 1)
  beta[2] ~ dnorm(0, 1)
  prec.e ~ dlnorm(0, 1)
}
Y = array(Y, dim=c(n, 1))
X = array(X, dim=c(dim(X), 1))
Y = dcdim(Y)
X = dcdim(X)
dat = list(Y=Y, X=X, n=n, ncl=1)
Normal.DC = dc.fit(data=dat, params=c("beta","prec.e"), model=Normal.model_dc,
  n.clones=c(1, 2, 5), unchanged="n", multiply="ncl")
summary(Normal.DC)
plot(Normal.DC)
pairs(Normal.DC)
```

#### Poisson log-link regression

We will now modify the code to conduct count data regression using the Poisson distribution and log-link.

```{r}
n = 30
X1 = rnorm(n)
X = model.matrix(~X1)
beta.true = c(0.5, 1)
link_mu = X %*% beta.true

# Log-linear regression model
mu = exp(link_mu)
Y = rpois(n, mu)

# MLE
MLE.est = glm(Y ~ X1, family="poisson")

# Bayesian analysis
Poisson.model = function(){
  # Likelihood 
  for (i in 1:n){
    mu[i] <- exp(X[i,] %*% beta)
    Y[i] ~ dpois(mu[i])
  }
  # Prior
  beta[1] ~ dnorm(0, 1)
  beta[2] ~ dnorm(0, 1)
}
dat = list(Y=Y, X=X, n=n)
Poisson.Bayes = jags.fit(data=dat, params="beta", model=Poisson.model)
summary(Poisson.Bayes)
plot(Poisson.Bayes)
pairs(Poisson.Bayes)

# MLE using data cloning
Poisson.model_dc = function(){
  # Likelihood 
  for (k in 1:ncl){
    for (i in 1:n){
      mu[i,k] <- exp(X[i,,k] %*% beta)
      Y[i,k] ~ dpois(mu[i,k])
    }
  }
  # Prior
  beta[1] ~ dnorm(0, 1)
  beta[2] ~ dnorm(0, 1)
}
Y = array(Y, dim=c(n, 1))
X = array(X, dim=c(dim(X), 1))
Y = dcdim(Y)
X = dcdim(X)
dat = list(Y=Y, X=X, n=n, ncl=1)
Poisson.DC = dc.fit(data=dat, params="beta", model=Poisson.model_dc,
  n.clones=c(1, 2, 5), unchanged="n", multiply="ncl")
summary(Poisson.DC)
plot(Poisson.DC)
pairs(Poisson.DC)
```

### Why use MCMC based Bayesian and data cloning?

1. Writing the model function is much more intuitive than writing the likelihood function, prior, etc. 
2. Do not need to do numerical integration or numerical optimization.
3. Data cloning overcomes multimodality of the likelihood function. Entire prior distribution essentially works as a set of starting values. In the usual optimization, starting values can be quite important when the function is not well behaved. By using data cloning, except for the global maximum, all the local maxima tend to 0. 
4. Asymptotic variance of the MLE is simple to obtain. It is also more stable than computing the inverse of the second derivative of the log-likelihood function numerically.

## Mixed Models

In the previous part, we reviewed the basic statistical concepts behind the likelihood inference and the Bayesian inference. We looked at how to write a JAGS model function for some linear and generalized linear regression models and use it in the package 'dclone' to get the Bayesian credible intervals as well as the frequentist confidence intervals based on the asymptotic normal distribution. We also discussed some of the reasons to use the MCMC approach to conducting statistical inference, either Bayesian or frequentist. 

We will now generalize the models to make them relevant to some complex practical situations. These are some of the situations where the analytical approaches to Bayesian and likelihood inference are difficult to impossible to implement. The question of estimability of the parameters becomes much more relevant but difficult to diagnose. The method of data cloning is particularly useful for diagnosing estimability of the parameters. You will notice that, although the models are much more complex, the coding component does not increase in complexity. We will also discuss prediction of missing data. 

### Detection error in occupancy studies (latent variables)

Let us revisit the occupancy model again. In practice, the assumption that you observe the occupancy status correctly is somewhat suspect. For example, if we are looking for a bird species, if the bird never sings or gives some sort of a cue, it is extremely difficult to know if they are there. Hence, even if the species is present, we may make an error and note that it is not present. This is called 'detection error'. How can we model this? 

Let $W_i$ denote the true status of the _i_-th cell. So in our previous notation, now $P(W_{i}=1)=\phi$. The observed value, generally denoted by $Y_i$ could be 1 or 0 depending on the true status. If we assume that the species are never misidentified, then we can write
$P(Y_i=1|W_{i}=1)=p$ and $P(Y_i=0|W_{i}=1)=1-p$. Moreover, $P(Y_i=0|W_{i}=0)=1$. In principle, we can also have misidentification. For example, a coyote could be mistaken for a wolf. But we will not discuss it here. It is a fairly simple extension of this model. Probability of detection is $p$ and probability of occupancy is $\phi$. How can we infer about these given the data? 

Notice that we only observe $Y_i$'s and not the $W_i$'s.  *The unobserved variable $W_i$ is called a latent variable.* 

To write down the likelihood function, we need to compute the distribution of the observed data $Y_i$. We can see that $P(Y_{i}=1)=p \phi$ and $P(Y_{i}=0)=(1-p) \phi$. We can write down the likelihood based on this. However, we are going to write this as a hierarchical model.  

Let us modify the previous code to see how this inference proceeds.

```{r}
library(dclone)

phi.true = 0.3 # occupancy
p.true = 0.7   # detectability
n = 30         # sample size
W = rbinom(n, 1 ,phi.true)   # true status
Y = rbinom(n, 1, W * p.true) # detections
```

#### Bayesian inference using JAGS and dclone

Step 1: WE need to define the model function. This is the critical component. 

```{r}
Occ.model = function(){
  # Likelihood: Latent variables are random variables.
  for (i in 1:n){
    W[i] ~ dbin(phi_occ, 1)
    Y[i] ~ dbin(W[i] * p_det, 1)
  }
  # Priors
  phi_occ ~ dbeta(1, 1)
  p_det ~ dbeta(1, 1)
}
```

Now we need to provide the data to the model and generate random numbers from the posterior. We will discuss different options later. 

```{r}
dat = list(Y=Y, n=n)
```

Following command will not work. But try it by removing the comments hash to see what happens.

```{r error=TRUE}
Occ.Bayes = jags.fit(data=dat, params=c("phi_occ","p_det"), model=Occ.model)
```

This did not quite work. When there are latent variables, many times, we have to start the process at the appropriate initial values. 

```{r}
ini = list(W=rep(1, n)) 
Occ.Bayes = jags.fit(data=dat, params=c("phi_occ","p_det"), model = Occ.model,
    inits=ini)
summary(Occ.Bayes)
plot(Occ.Bayes)
```

This seems to work quite well! But our answers are quite weird (We know the truth!). Let us plot the two dimensional (joint) distribution of the parameters.

```{r}
pairs(Occ.Bayes)
```

This suggests that the posterior distribution is banana shaped. But just looking at these plots, we cannot say for sure if our answers are correct or not. 

Should we, then, accept the answers? Not so fast. Let us look at the model again. It is clear that we can estimate the product $p \phi$ given the data. But decomposing this product in $p$ and $\phi$ is impossible. This is called 'non-estimability'.

In this case, this also is non-identifiability. There are several combinations of $p$ and $\phi$ that lead to the same $p \phi$ and hence the same distribution of the observed data. Such situations are not uncommon when dealing with the hiearchical models in general, and measurement error models in particular.

We should not make any inferences about the probability of occupancy based on these data. You can change the priors and see what happens to the posteriors. You might find it interesting and educational. 

**Non-estimability: If there are two or more values in the parameter space that lead to identical likelihood value, such values are called 'non-estimable'.**

Note: You may recall from the linear regression that if the covariates are perfectly correlated with each other, the regression coefficients are non-estimable. If covariate $X_1$ is perfectly correlated with $X_2$, these covariates separately give no additional information. 

#### Bayesian result and its data cloning version

If the posterior distribution converges to a non-degenerate distribution as the sample size increases, it implies that set of parameters is non-estimable. 

If the posterior distribution converges to a non-degenerate distribution as the number of clones increases, it implies that set of parameters is non-estimable.

An immediate consequence of this result is that the variance of the posterior distribution does not converge to 0 (instead it converges to some positive number). 

Let us modify our data cloning code to see what happens. 

```{r}
Occ.model.dc = function(){
  # Likelihood 
  for(k in 1:ncl){
    for (i in 1:n){
        W[i,k] ~ dbin(phi_occ, 1)
        Y[i,k] ~ dbin(W[i,k] * p_det, 1)
    }
  }
  # Prior
  phi_occ ~ dbeta(1, 1)
  p_det ~ dbeta(1, 1)
}
```

We need to turn the original data into an array. And we need to add another index `ncl` for the cloned dimension. It gets multiplied by the number of clones.

```{r}
Y = array(Y, dim=c(n, 1))
Y = dcdim(Y)
dat = list(Y=Y, n=n, ncl=1)
```

As previously, we need to initiate the `W`'s. 

```{r}
ini = list(W=array(rep(1, n), dim=c(n, 1)))
```

We need to clone these initial values as well. You should always check if this is doing the right job.

```{r}
initfn = function(model, n.clones){
  W=array(rep(1, n), dim=c(n, 1))
  list(W=dclone(dcdim(W), n.clones))
}
initfn(n.clones=2)
```

Let's run data cloning.

```{r}
Occ.DC = dc.fit(data=dat, params=c("phi_occ","p_det"), model=Occ.model.dc,
    n.clones=c(1, 2, 5), unchanged="n", multiply="ncl",
    inits=ini, initsfun=initfn)
summary(Occ.DC)
plot(Occ.DC)
pairs(Occ.DC)
```

There are a couple of diagnostic tools available in 'dclone' for the non-estimability issue. 

```{r}
dcdiag(Occ.DC)
dcdiag(Occ.DC) |> plot()
```


1. Check if the `lambda.max` is converging to zero. The rate at which this converges to zero should be approximately 1/_K_.
2. If the variance is converging to 0 but the rate is different than 1/_K_, it implies the asymptotics is of a different order. If you find such an example in your work, please let me know. 
3. Check the pairs plot to see if the likelihood function is converging to a manifold instead of a single point. 

**Availability of the estimability diagnostics is one of the most important features of data cloning. It will warn you if your scientific inferences could be misleading.**

If the parameters are non-estimable, the only recourse one has is to change the model (add assumptions or collect different kind of data). There is always a possibility that, although the full parameter space may not be estimable, a function of the parameter might be estimable. If such a function is also of scientific interest, we can safely conduct scientific inferences based on estimates of such a function of the parameters.

#### Replicate surveys

Suppose we visit the same cell several times. Assume that the visits are independent of each other and the true occupancy status remains the same throughout these surveys, then we can estimate the parameters. The model can be written as a hierarchical model:

- Hierarchy 1: $W_{i}\sim Bernoulli(\phi)$
- Hierarchy 2: $Y_{ij}|W_{i}=w_{i}\sim Bernoulli(p w_{i})$

Notice that hierarchy 2 depends on hierarchy 1 result. 

We can easily modify the earlier code to allow for multiple surveys. We will do such a modification with two surveys for each cell. 

```{r}
phi.true = 0.3
p.true = 0.7
n = 30
v = 2 # number of visits
W = rbinom(n, 1, phi.true)
Y = NULL
for (j in 1:v)
    Y <- cbind(Y, rbinom(n, 1, W * p.true))
```

#### Bayesian inference using JAGS and dclone

Step 1: we need to define the model function.

```{r}
Occ.model = function(){
  # Likelihood: Latent variables are random variables.
  for (i in 1:n){
    W[i] ~ dbin(phi_occ, 1)
    for (j in 1:v){
        Y[i,j] ~ dbin(W[i] * p_det, 1)}
  }
  # Priors
  phi_occ ~ dbeta(1, 1)
  p_det ~ dbeta(1, 1)
}
```

Now we need to provide the data to the model and generate random numbers from the posterior. We will discuss different options later. 

```{r}
dat = list(Y=Y, n=n, v=v)
ini = list(W=rep(1, n))

Occ.Bayes = jags.fit(data=dat, params=c("phi_occ","p_det"), model=Occ.model,
    inits=ini)
summary(Occ.Bayes)
plot(Occ.Bayes)
pairs(Occ.Bayes)
```

These are nice unimodal posterior distributions.

We can modify the data cloning code to check if the parameters are, in fact, estimable. 

```{r}
Occ.model.dc = function(){
  # Likelihood
  for(k in 1:ncl){
    for (i in 1:n){
      W[i,k] ~ dbin(phi_occ, 1)
      for (j in 1:v){
        Y[i,j,k] ~ dbin(W[i,k] * p_det, 1)
      }
    }
  }
  # Prior
  phi_occ ~ dbeta(1, 1)
  p_det ~ dbeta(1, 1)
}
```

We need to turn the original data into an array. 
And we need to add another index `ncl` for the cloned dimension. It gets multiplied by the number of clones.

```{r}
Y = array(Y, dim=c(n,v,1))
Y = dcdim(Y)
dat = list(Y=Y, n=n, v=v, ncl=1)

# As previously, we need to initiate the W's. 
ini = list(W=array(rep(1, n), dim=c(n, 1)))
# We need to clone these initial values as well. You should always check if this is doing the right job.
initfn =function(model, n.clones){
  W=array(rep(1,n), dim=c(n,1))
  list(W=dclone(dcdim(W), n.clones))
}
Occ.DC = dc.fit(data=dat, params=c("phi_occ","p_det"), model=Occ.model.dc,
    n.clones=c(1, 2, 5),
    unchanged=c("n","v"), multiply="ncl",
    inits=ini, initsfun=initfn)
summary(Occ.DC)
plot(Occ.DC)
pairs(Occ.DC)

dcdiag(Occ.DC)
plot(dcdiag(Occ.DC))
```

### Random effects in regression: Why and when? 

We have shown how hierarchical models can be used to deal with measurement error. Now we will look at a few examples where we use them to combine data across several studies. 

We will start with a simple (but extremely important) example that started the entire field of mixture as well as hierarchical models (Neyman and Scott, 1949).

Researchers in animal husbandary wanted to know how to improve the stock of animals such as milk cows and bulls. This played an important role in the 'white revolution' that lead to improving the nutrition in many countries. Following is a somewhat made up and highly simplified situation. 

Suppose we have _n_ cows. We want to know which cows have good genetic potential that can be passed on to the next generation. Each cow might have only a few calves. We measure the amount of milk by each calf. 

Let $Y_ij$ be the amount of milk produced by the j-th calf of the i-th cow. We can consider a linear model that uses the 'cow effect' (genetic) and 'environmental effect' to explain the amount of milk.
This is same as one way ANOVA model.

$Y_{ij}=\mu+\alpha_{i}+\epsilon_{ij}$

Under the usual Gaussian error structure, we know that

$Y_{ij}\sim N(\mu_{i},\sigma^{2})$ where $i=1,2,...,n$ and $j=1,2$ ($\mu_{i}=\mu + \alpha_i$).

There are $2 n$ observations and $n+1$ parameters. The number of parameters increases at the same rate as the number of observations. Note that the ratio of parameters to observations converges to 0.5. Roughly speaking, for the MLE to work, this ratio needs to go to 0. Generally the number of parameters is fixed and hence this condition is satisfied.

We simply do not have much information about each $\mu_i$ as there are only two observations corresponding to it. It also turns out that the ML estimator of $\sigma^2$ converges to $0.5*\sigma^2$. Hence it is not consistent even though the number of observations corresponding to it do converge to infinity. This was a major blow to the theory of maximum likelihood. Although it turns out Fisher had implicitly answered it a decade before this paper.

How can we reduce the number of parameters? 

1. If there are covariates such as weight of the mother, mother's milk production are available, we can model $\mu_{i}=X_{i}\beta$. 
2. Suppose covariates are not available or difficult to assess what to use as a covariate. If we assume that cows are kind of similar to each other, then we can assume that they come from a population of cows such that $\mu_{i}\sim N(\mu,\tau^{2})$. It turns out, under such an assumption, we can estimate the parameters $(\mu,\sigma^2,\tau^2)$ consistently. 

This model is a hierarchical model.

- Hierarchy 1: $Y_{ij}|\mu_i\sim N(\mu_{i},\sigma^{2})$ 
- Hierarchy 2: $\mu_{i}\sim N(\mu,\tau^{2})$

For a Bayesian approach, we put priors on the three parameters. This forms the third hierarchy. 

This simple model can be used in many different situations. 

1. Measurement error in covariates in regression
2. Random intercept regression model to account for missing covariates
3. Kalman filter models for time series with measurement error are of the same kind with a bit more complexity as we will see the third part. 

Let us see what makes it a difficult model to analyze using the likelihood approach.

In order to write down the likelihood function, we need to compute the marginal distribution of the observations. Remember $\mu_i$ are not observed. Hence we have to integrate over them.

\[
f(y_{ij};\mu,\sigma^{2},\tau^{2})=\int f(y_{ij}|\mu_{i})g(\mu_{i})d\mu_{i}
\]

Again, this is not a precise statement but it gives you the idea. This integral is one dimensional and hence can be computed analytically and also numerically. However, in many cases the dimension of the integral is quite large and hence neither of these solutions are available. In some cases, one can obtain Laplace approximation to this integral (INLA and related methods rely on this approximation). The most general approach is based on the MCMC algorithm.

```{r}
n = 30
mu_true = 2
tau_true = 0.5
sigma_true = 1

mu = rnorm(n, mu_true, tau_true)
Y = cbind(
    rnorm(n, mu, sigma_true),
    rnorm(n, mu, sigma_true))
```

We will write the Bayesian model first. Remember the normal distribution is defined in terms of the precision (inverse of the variance).

```{r}
LM_Bayes = function(){
  # Likelihood
  for (i in 1:n){
    mu[i] ~ dnorm(mu.t, prec_tau)
    for (j in 1:2){
      Y[i,j] ~ dnorm(mu[i], prec_sigma)
    }
  }
  # Priors
  mu.t ~ dnorm(0, 0.01)
  prec_tau ~ dgamma(0.1, 0.1)
  prec_sigma ~ dgamma(0.1, 0.1)
  # Parameters on the natural scale
  tau <- sqrt(1/prec_tau)
  sigma <- sqrt(1/prec_sigma)
}
```

Data in array form for data cloning purpose

```{r}
Y = as.array(Y, dim=c(n,2,1))
Y = dcdim(Y)

dat = list(Y=Y, n=n)
LM_Bayes_fit = jags.fit(data=dat, params=c("mu.t","tau","sigma"), model=LM_Bayes)
summary(LM_Bayes_fit)
```

We will modify it to do data cloning. This is useful to assure us that the parameters are estimable. It is also important for making it invariant to the choice of the priors and parameterization.

```{r}
LM_DC = function(){
  # Likelihood
  for (k in 1:ncl){
    for (i in 1:n){
      mu[i,k] ~ dnorm(mu.t, prec_tau)
      for (j in 1:2){
        Y[i,j,k] ~ dnorm(mu[i,k], prec_sigma)
      }
    }
  }
  # Priors
  mu.t ~ dnorm(0, 0.01)
  prec_tau ~ dgamma(0.1, 0.1)
  prec_sigma ~ dgamma(0.1, 0.1)
  # Parameters on the natural scale
  tau <- sqrt(1/prec_tau)
  sigma <- sqrt(1/prec_sigma)
}
```

Data in array form for data cloning purpose.

```{r}
Y = array(Y, dim=c(n,2,1))
Y = dcdim(Y)

dat = list(Y=Y, n=n, ncl=1)
LM_DC_fit = dc.fit(data=dat, params=c("mu.t","tau","sigma"), model=LM_DC,
    n.clones=c(1, 2, 5), unchanged="n", multiply="ncl")
summary(LM_DC_fit)
pairs(LM_DC_fit)
dcdiag(LM_DC_fit)
plot(dcdiag(LM_DC_fit))
```

The best thing about the MCMC approach is that we can modify the prototype program to do Generalized linear mixed models. 

Let us see how we can change the program to do Poisson regression with random intercepts. This is useful for accounting for missing covariates in the usual Poisson regression. This can also be used to account for site effect in abundance surveys. 

Mathematically the model is:

- Hierarchy 1: $log(\lambda_{i})\sim N(log(\lambda),\tau^{2})$
- Hierarchy 2: $Y_{i}|\lambda_{i}\sim Poisson(\lambda_{i})$

```{r}
n = 30
mu_true = 2
tau_true = 0.5
sigma_true = 1

mu = rnorm(n, mu_true, tau_true)
Y = rpois(n, exp(mu))
```

We will write the Bayesian model first. Remember the normal distribution is defined in terms of the precision (inverse of the variance).

```{r}
GLMM_Bayes = function(){
  # Likelihood
  for (i in 1:n){
    mu[i] ~ dnorm(mu.t, prec_tau)
    Y[i] ~ dpois(exp(mu[i]))
  }
  # Priors
  mu.t ~ dnorm(0, 0.01)
  prec_tau ~ dgamma(0.1, 0.1)
  # Parameters on the natural scale
  tau <- sqrt(1/prec_tau)
  lambda <- exp(mu.t)
}

dat = list(Y=Y, n=n)
GLMM_Bayes_fit = jags.fit(data=dat, params=c("lambda","tau"), model=GLMM_Bayes)
summary(GLMM_Bayes_fit)
```

As usual, we can turn the crank for data cloning to check the estimability of the parameters. 

```{r}
GLMM_DC = function(){
  # Likelihood
  for (k in 1:ncl){
    for (i in 1:n){
      mu[i,k] ~ dnorm(mu.t,prec_tau)
      Y[i,k] ~ dpois(exp(mu[i,k]))
    }
  }
  # Priors
  mu.t ~ dnorm(0, 0.01)
  prec_tau ~ dgamma(0.1, 0.1)
  # Parameters on the natural scale
  tau <- sqrt(1/prec_tau)
  lambda <- exp(mu.t)
}

# Data in array form for data cloning purpose

Y = array(Y, dim=c(n,1))
Y = dcdim(Y)

dat = list(Y=Y, n=n, ncl=1)
GLMM_DC_fit = dc.fit(data=dat, params=c("lambda","tau"), model=GLMM_DC,
    n.clones=c(1, 2, 5), unchanged="n", multiply="ncl")
summary(GLMM_DC_fit)

exp(mu_true)
tau_true
```

#### Estimating the effect of a treatment from multi-center clinical trials

In clinical trials, we are interested in estimating the effect of the treatment. One of the simplest forms of clinical trial is where we split a group of patients in two groups randomly. One of the group gets the treatment and the other gets a placebo.

We can then estimate the difference in the outcomes. This may be done using a simple t-test if the outcome is a continuous measurement. If the patients are quite different from each other in terms of say age, blood pressure or some such physical characteristics that may affect the outcome, we adjust them by using a regression approach. We include these other covariates and the treatment/control indicator variable in the model. The effect of the treatment after adjusting for other covariates can be studied using such a regression model. 

Often in practice, we may not have access to such covariates. Using random intercept model in regression is one way out of such a situation. In this case, we do consider the differences between patients but without ascribing them to any specific, known values of the covariates. The model one may consider is:

\[
Y_{i}=\beta_{0i}+\beta I_{(Treatment)}+\epsilon_{i}
\]

As we have seen in the previous example (Neyman-Scott problem), this model is non-estimable. One way to make it estimable is by using a hierarchical structure: 

Hierarchy 2: $\beta_{0i}\sim N(\beta_{0},\tau^{2})$

Homework: Check the validity of the following statement without doing any mathematics. You can use data cloning to do that. 

This leads to estimability for $\beta$, the parameter of interest. Although it does not lead to estimation of the variances $(\tau,\sigma)$.

> An approach not described in this course: We can use profile likelihood for the parameter $\beta$. This eliminates the 'nuisance parameters' $\beta_0,\tau,\sigma$. Computing the profile likelihood and quantification of uncertainty for inferences based on it for hierarchical models can be tackled using data cloning. This could be another course! 

Suppose the outcome is binary, survival for 5 years vs. failure before 5 years. In this case, a convenient model is a binary regression model such as a Logistic regression model. 

Hierarchy 1: 

\[
P(Y_{i}=1)=\frac{exp(\beta_{0i}+\beta I_{(Treatment)})}{1+exp(\beta_{0i}+\beta I_{(Treatment)})}
\]

Hierarchy 2: $\beta_{0i}\sim N(\beta_{0},\tau^{2})$

This is an example of a Generalized Linear Mixed Model. Fortunately, for this model all parameters are estimable as we will see using data cloning. The random intercept here could be accounting for differences in the clinical centers (hospitals), assuming we have only two patients, one in control and one in the treatment group. If we have multiple patients in each group, we may include another random effect to account for differences in the patients within each group. For example, we may consider a model:

Hierarchy 1: 

\[
P(Y_{ij}=1)=\frac{exp(\beta_{0i}+\beta I_{(Treatment)}+\beta_{1j})}{1+exp(\beta_{0i}+\beta I_{(Treatment)}+\beta_{1j})}
\]

Hierarchy 2: 
$\beta_{0i}\sim N(\beta_{0},\tau^{2})$,
$\beta_{1j}\sim N(\beta_{1},\tau^{2})$

We can include interactions between random effects and so on. We will not go into these complex models in this course. 

Let us see how one can modify the prototype program to analyze the random intercept Logistic regression model.

Each center has one control and one treatment patient.

```{r}
n = 300      # Number of clinical centers
mu_true = 0
tau_true = 0.5
delta = 1   # Treatment effect
mu = rnorm(n, mu_true, tau_true)
Y = cbind(
    rbinom(n, 1, plogis(mu)),
    rbinom(n, 1, plogis(mu+delta)))
```

We will write the Bayesian model first. Remember the normal distribution is defined in terms of the precision (inverse of the variance).

```{r}
GLMM_Bayes = function(){
  # Likelihood
  for (i in 1:n){
    mu[i] ~ dnorm(mu.t,prec_tau)
    Y[i,1] ~ dbin(ilogit(mu[i]),1)
    Y[i,2] ~ dbin(ilogit(mu[i]+delta),1)
  }
  # Priors
  mu.t ~ dnorm(0, 0.01)
  prec_tau ~ dgamma(0.1, 0.1)
  delta ~ dnorm(0, 0.1)
  # Parameters on the natural scale
  tau <- sqrt(1/prec_tau)
}

dat = list(Y=Y, n=n)
GLMM_Bayes_fit = jags.fit(data=dat, params=c("delta","mu.t","tau"), model=GLMM_Bayes)
summary(GLMM_Bayes_fit)
```

We will modify this to get the MLE using data cloning.

```{r}
GLMM_DC = function(){
  # Likelihood
  for (k in 1:ncl){
    for (i in 1:n){
      mu[i,k] ~ dnorm(mu.t,prec_tau)
      Y[i,1,k] ~ dbin(ilogit(mu[i,k]),1)
      Y[i,2,k] ~ dbin(ilogit(mu[i,k]+delta),1)
    }
  }
  # Priors
  mu.t ~ dnorm(0, 0.01)
  prec_tau ~ dgamma(0.1, 0.1)
  delta ~ dnorm(0, 0.1)
  # Parameters on the natural scale
  tau <- sqrt(1/prec_tau)
}

Y = array(Y, dim=c(dim(Y),1))
Y = dcdim(Y)
dat = list(Y=Y, n=n, ncl=1)

GLMM_DC_fit = dc.fit(data=dat, params=c("delta","mu.t","tau"), model=GLMM_DC,
    n.clones=c(1, 2, 5), unchanged="n", multiply="ncl")

summary(GLMM_DC_fit)
pairs(GLMM_DC_fit)
dcdiag(GLMM_DC_fit)
plot(dcdiag(GLMM_DC_fit))
```

Following is a real data set on multi-center clinical trials. Number of patients in the treatment are `"nt"` and those who survived are `"rt"`. Similarly `"nc"` are the number of patients in the control group and `"rc"` are the ones that survived. 

```{r}
OR.data = list(
    "rt" =
        c(3, 7, 5, 102, 28, 4, 98, 60, 25, 138, 64, 45, 9, 57, 25, 33, 
        28, 8, 6, 32, 27, 22),
    "nt" =
        c(38, 114, 69, 1533, 355, 59, 945, 632, 278, 1916, 873, 263, 
        291, 858, 154, 207, 251, 151, 174, 209, 391, 680),
    "rc" =
        c(3, 14, 11, 127, 27, 6, 152, 48, 37, 188, 52, 47, 16, 45, 31, 
        38, 12, 6, 3, 40, 43, 39),
    "nc" =
        c(39, 116, 93, 1520, 365, 52, 939, 471, 282, 1921, 583, 266, 
        293, 883, 147, 213, 122, 154, 134, 218, 364, 674),
    "Num" =
        22)
```

The model functions for DD are as follows.

```{r}
DC.MLE.fn = function() {
  for (k in 1:ncl){
    for (i in 1:Num) {
      rt[i,k] ~ dbin(pt[i,k], nt[i,k])
      rc[i,k] ~ dbin(pc[i,k], nc[i,k])
      logit(pc[i,k]) <- mu[i,k] 
      logit(pt[i,k]) <- mu[i,k] + delta
      mu[i,k] ~ dnorm(alpha,tau1)
    }
  }
  # Priors 
  parms ~ dmnorm(MuP, PrecP)
  delta <- parms[1]
  tau1 <- exp(parms[3])
  sigma1 <- 1/sqrt(tau1)
  alpha <- parms[2]
}

# Analysis
rt = dcdim(array(OR.data$rt, dim=c(22,1)))
nt = dcdim(array(OR.data$nt, dim=c(22,1)))
rc = dcdim(array(OR.data$rc, dim=c(22,1)))
nc = dcdim(array(OR.data$nt, dim=c(22,1)))
Num = OR.data$Num

dat = list(rt=rt, rc=rc, nt=nt, nc=nc, Num=Num, ncl=1, 
    MuP=rep(0, 3), PrecP=diag(0.01, 3, 3))

DC.MLE = dc.fit(data=dat, params="parms", model=DC.MLE.fn,
    n.clones=c(1, 4, 16),
    multiply="ncl", unchanged=c("Num","MuP","PrecP"),
    n.chains=5, n.update=1000, n.iter=5000, n.adapt=2000)

# Check the convergence and MLE estimates etc.
summary(DC.MLE)
dctable(DC.MLE)
dcdiag(DC.MLE)
```

It should be clear by now that we can modify any Bayesian analysis to get Maximum likelihood estimate quite easily by adding one dimension to the data and a do loop over the clones. 

In the next part, we will discuss how to analyzed time series data. 

## Time series

In this part, we will demonstrate how one can use MCMC and data cloning to analyze time series data sets. These examples can be extended to longitudinal data sets, spatial data sets quite easily.

These models also tend to have missing observations. The code can be modified easily to account for the missing observations for estimating the parameters. We may also want to _predict_ the values of the missing observations. We will demonstrate how to predict missing data or forecast future observations. 

### Auto-regression of order 1 (AR(1) process)

This is one of the most basic time series models. The AR(1) model can be written as:

\[
Y_{i}=\rho Y_{i-1}+\epsilon_{i}
\]

where $i=1,2,...,n$ and $\epsilon_i\sim N(0,\sigma^{2})$ are indepedent random variables (_N_ is shorthand for the Normal distribution). 

This model says that the next year's value is related to the past year's value. That is, the value in the past year is a good predictor for the next year's value. Hence the term 'auto-regression'. This model can be used to model many different phenomena that proceed in time. For example, tomorrow's temperature can be predicted using today's temperature (except in Alberta!). Next day's stock price is likely to be related to today's price and so on. 

This model can be modified to include covariates. Hence, we can write:

\[
Y_{i}=X_{i}\beta+\rho(Y_{i-1}-X_{i-1}\beta)+\epsilon_{i}
\]

This allows for correlated environmental noise in regression.

This model has been used to model changes in wildlife populations. It is useful in epidemiology and so on. Many econometric models are derived from this basic model.

Of course, reality is most of the times more complicated than this model. For example, one may not observe the response without error. This is called an observation error. We may not (most of the times, we will not) observe the true population size but only an estimate (or an index) of the true population size. Thus, the observed value is not the true response. Such cases are modelled using a hierarchical structure:

- Hierarchy 1 (True state model): $X_{i}=\rho X_{i-1}+\epsilon_{i}$
- Hierarchy 2 (Observation model): $Y_i = X_i + \eta_i$ where $\eta_i$ is observation error and $\eta_i\sim N(0,\tau^2)$ are independent random variables

This is what is called a 'Kalman filter' after a famous Hungarian electrical engineer, Professor Rudolf Kalman. This is a particular case of the model class 'State space models'. They consist of at least two hierarchies: one models the true underlying phenomenon and the other the observation process that models the error due to observation process. 

Under the Normal distribution assumption, the mathematics can be worked out for the simple linear model to conduct the likelihood inference. But once we enter the non-linear time series modelling or non-Gaussian observation processes, mathematics become nearly impossible. We will see an example of this a bit later. It will also illustrate why the MCMC algorithm is considered one of the greatest inventions in modern science. 

For the time being, let us avoid all the mathematics and see if we can use JAGS and dclone to conduct the statistical analysis. 

> No math please!!! -- this is our motto.

```{r}
library(dclone)
T_max = 200  # Number of time steps
rho = 0.3
sigma.e = 1
tau.e = 0.25
X = rep(0, T_max)

# This is the stationary distribution for the AR(1) process
X[1] = rnorm(1, 0, sigma.e/sqrt(1-rho^2))

for (t in 2:(T_max)){
  X[t] = rho*X[t-1] + rnorm(1, 0, sigma.e)
}

# Add observation error
Y = rnorm(length(X), X, tau.e)

plot(Y, type="l", lty=2, xlab="Time", ylab="Value")
lines(X)
legend("topleft", lty=c(1, 2), legend=c("True", "Observed"))
```

We will start with the Bayesian approach.

```{r}
AR1_Bayes_model = function(){
  # Likelihood
  prec.1 <- (1-rho*rho) * prec.e
  X[1] ~ dnorm(0, prec.1)
  Y[1] ~ dnorm(X[1], prec.t)
  for (t in 2:T_max){
    mu[t] <- rho * X[(t-1)]
    X[t] ~ dnorm(mu[t], prec.e)
    Y[t] ~ dnorm(X[t], prec.t)
  }
  # Priors
  rho ~ dunif(-1, 1)
  prec.e ~ dgamma(0.1, 0.1)
  prec.t ~ dgamma(0.1, 0.1)
}
```

Get the data and run the analysis.

```{r}
Y = as.vector(Y)
dat = list(Y=Y, T_max=T_max)
ini = list(X=Y)
AR1_Bayes_fit = jags.fit(data=dat, params=c("rho","prec.e","prec.t"), model=AR1_Bayes_model)

summary(AR1_Bayes_fit)
plot(AR1_Bayes_fit)
```

We will modify this to get the MLE using data cloning.

```{r}
AR1_DC_model = function(){
  # Likelihood
  for (k in 1:ncl){
    X[1,k] ~ dnorm(0, prec.1)
    Y[1,k] ~ dnorm(X[1,k], prec.t)
    for (t in 2:T_max){
        mu[t,k] <- rho*X[(t-1),k]
        X[t,k] ~ dnorm(mu[t,k], prec.e)
        Y[t,k] ~ dnorm(X[t,k], prec.t)
    }
  }
  # Priors
  rho ~ dunif(-1, 1)
  prec.e ~ dgamma(0.1, 0.1)
  prec.t ~ dgamma(0.1, 0.1)
  prec.1 <- (1-rho*rho) * prec.e
}
```

Get the data and run the analysis with data cloning.

```{r}
Y = array(Y, dim=c(length(Y), 1))
Y = dcdim(Y)
dat = list(Y=Y, T_max=T_max, ncl=1)
ini = list(X=Y)
initfn = function(model, n.clones){
  return(list(X=dclone(Y, n.clones)))
}
AR1_DC_fit = dc.fit(data=dat,params=c("rho","prec.t","prec.e"),model=AR1_DC_model,
    unchanged="T_max",multiply="ncl",
    n.clones=c(1, 10, 20),
    inits=ini, initsfun=initfn)

summary(AR1_DC_fit)
dcdiag(AR1_DC_fit)
plot(dcdiag(AR1_DC_fit))
```

Try running the above code when true `rho=0`.

- Are the parameters estimable? 
- Does the Bayesian approach tell you that? 

Without the estimability diagnostics, you could be easily mislead by the Bayesian approach. You will merrily go around with the scientific inference when the parameters are not estimable.


### Different types of measurement errors

1: Clipped or Censored time series

Suppose the underlying process is AR(1) but the observed process is a clipped process such that it is 1 if $X$ is positive and 0 if $X$ is negative. This is called a clipped time series. Similarly you may observe $Y$ to belong to an interval. This is an interval censored data. The above model can be modified to accommodate such a process. An easier way to model binary, count or proportion time series is as follows.

#### Modelling binary and count data time series

For modelling binary time series, we can consider the observation process as:

$Y_{t}\sim Bernoulli(p_{t})$ where $log(\frac{p_{t}}{1-p_{t}})=\gamma*X_{t}$

For modelling count data time series, we can consider

$Y_{t}\sim Poisson(\lambda_{t})$ where $log\lambda_{t}=\gamma*X_{t}$.

This is a time series generalization of the GLMM that we considered before.  

_Caveat_: It is extremely important that you check for the estimability of the parameters for these models. Because of clipping and other observation processes, you are more likely to run into estimability issues. As far as we know, data cloning is the only method that allows estimability diagnostics as part of the estimation process. See Lele ([2010](https://github.com/datacloning/workshop-2023-edmonton/blob/main/docs/lele-2010-built-on-sand.pdf)). 

#### Non-linear time series analysis

Now we will consider a non-linear time series model, Beverton-Holt growth model, that is commonly used in ecology. 

In ecology and population biology, one wants to understand how abundance changes over time. Following Malthus' thinking, it is also evident that, in a finite environment, abundance cannot increase without limit.

Thus, the growth is usually exponential at the beginning (low population, ignore the Allee effect for now) and then it slows down as we approach the carrying capacity. One commonly used model is the Beverton-Holt model (discrete analog to the continuous Logistic model):

Let $log(N_{t})=X_{t}$ where $N_t$ is the abundance at time $t$. A general form for the population growth models is: 

$X_{t}=m_{t}+\sigma Z_{t}$

where $Z_t$ is $Normal(0,1)$ random variable. This is similar to the AR(1) process but with a non-linear mean structure. 

If $m_{t}=log(\lambda)+x_{t}-log(1+\beta N_{t})$, the population growth model is called the Beverton-Holt model. It has an upper limit $K=\frac{\lambda-1}{\beta}$ called the Carrying capacity, the maximum population size that can be attained (with some perturbation). 

In practice, we usually have to conduct some sampling to _estimate_ the abundance. Hence there is measurement error. We can represent this process by using hierarchical model:

- Hierarchy 1: Process model, $X_{t}|X_{t-1}=x_{t-1}\sim Normal(m(x_{t-1}),\sigma^{2})$
- Hierarchy 2: Observation model, $Y_{t}|N_{t}\sim Poisson(N_{t})$

One can use other observation models as well. We can write the likelihood function for this using a $T_{max}$ (length of the time series) dimensional integral. If the time series is of length 30, this will be a 30 dimensional integral. In order to compute the MLE, we will need to evaluate this integral repeatedly until the numerical optimization routine converges. This is a nearly impossible task.

The code to analyze this non-linear time series with non-Gaussian observation error can be written as follows.

```{r}
BH_Bayes_fn= function() {
  # Likelihood
  X[1] ~ dnorm(mu0, 1 / sigma^2) # Initial condition
  for(i in 2:(n+1)){
    Y[(i-1)] ~ dpois(exp(X[i])) 
    X[i] ~ dnorm(mu[i], 1 / sigma^2) 
    mu[i] <- X[(i-1)] + log(lambda) - log(1 + beta * exp(X[(i-1)]))
  }

  # Priors on model parameters: They are on the real line.
  ln.beta ~ dnorm(0, 0.1) 
  ln.sigma ~ dnorm(0, 0.1)
  ln.tmp ~ dnorm(0, 0.1)

  # Parameters on the natural scale
  beta <- exp(ln.beta)
  sigma <- exp(ln.sigma)
  tmp <- exp(ln.tmp)
  lambda <- tmp + 1
  mu0 <- log(2)  + log(lambda) - log(1 + beta * 2)
}
```

Gause's _Paramecium_ data.

```{r}
Paramecium = c(17,29,39,63,185,258,267,392,510,570,650,560,575,650,550,480,520,500)
plot(Paramecium, type="b")
```

Bayesian analysis.

```{r}
Y = Paramecium
dat = list(n=length(Y), Y=Y)
BH_Bayes_fit = jags.fit(data=dat, params=c("ln.tmp","ln.beta","ln.sigma"), model=BH_Bayes_fn)

summary(BH_Bayes_fit)
```

We can easily modify this program to obtain the MLE and its asymptotic variance. 

```{r}
BH_DC_fn= function() {
  # Likelihood
  for (k in 1:ncl) {
    for(i in 2:(n+1)){
      Y[(i-1), k] ~ dpois(exp(X[i, k])) 
      X[i, k] ~ dnorm(mu[i, k], 1 / sigma^2) 
      mu[i, k] <- X[(i-1), k] + log(lambda) - log(1 + beta * exp(X[(i-1), k]))
    }
    X[1, k] ~ dnorm(mu0, 1 / sigma^2) 
  }

  # Priors on model parameters: They are on the real line.
  ln.beta ~ dnorm(0, 0.1) 
  ln.sigma ~ dnorm(0, 0.1)
  ln.tmp ~ dnorm(0, 0.1)

  # Parameters on the natural scale
  beta <- exp(ln.beta)
  sigma <- exp(ln.sigma)
  tmp <- exp(ln.tmp)
  lambda <- tmp + 1
  mu0 <- log(2)  + log(lambda) - log(1 + beta * 2)
}
```

Assemble the data and fit with data cloning.

```{r}
Y = array(Y, dim=c(length(Y), 1))
Y = dcdim(Y)
dat = list(ncl=1, n=18, Y=Y)

n.clones = c(1, 5, 10)
params = c("ln.tmp", "ln.beta", "ln.sigma")
BH_MLE = dc.fit(data=dat, params=params, model=BH_DC_fn,
    n.clones=n.clones,
    multiply="ncl", unchanged="n",
    n.chains=5, n.update=1000, n.iter=5000, n.adapt=2000)

dcdiag(BH_MLE)
```

Parameters for this model are estimable. It will be interesting to see if one can use Negative Binomial distribution (one additional parameter) instead of the Poisson distribution. Are the parameters still estimable? (TBD!!!)

### Prediction

We are also interested in predicting the true population abundances as well as forecasting the future trajectory of the abundances. This is quite easy under the Bayesian paradigm. The frequentist paradigm involves an additional step. 

Let us see how to use the Bayesian paradigm and MCMC to do this. In the Bayesian paradigm, there is no difference between parameters and the unobserved states. They both are considered random variables. 

On the other hand, in the frequentist paradigm parameters are fixed but unknown (not random) whereas the unobserved states are true random variables.

We (the instructors) consider these to be different.

1. Information about the parameters converges to infinity as the sample size increases. Thus, we can _estimate_ them with high degree of confidence. The _confidence_ intervals shrink as we increase the sample size.
2. Information about the states (random variables) does not converge to infinity as the sample size increases. The _prediction_ intervals do not shrink. 

This should be familiar to most of you from your regression class. 

```{r}
BH_Bayes_fn= function() {
  # Likelihood
  X[1] ~ dnorm(mu0, 1 / sigma^2) # Initial condition
  N[1] <- exp(X[1])
  for(i in 2:(n+1)){
    Y[(i-1)] ~ dpois(exp(X[i])) 
    X[i] ~ dnorm(mu[i], 1 / sigma^2) 
    mu[i] <- X[(i-1)] + log(lambda) - log(1 + beta * exp(X[(i-1)]))
    N[i] <- exp(X[i])
  }

  # Priors on model parameters: They are on the real line.
  ln.beta ~ dnorm(0, 0.1) 
  ln.sigma ~ dnorm(0, 0.1)
  ln.tmp ~ dnorm(0, 0.1)

  # Parameters on the natural scale
  beta <- exp(ln.beta)
  sigma <- exp(ln.sigma)
  tmp <- exp(ln.tmp)
  lambda <- tmp + 1
  mu0 <- log(2)  + log(lambda) - log(1 + beta * 2)
}

# Gause's data 
Y = Paramecium
dat = list(n=length(Y), Y=Y)
BH_Bayes_fit = jags.fit(data=dat, params=c("N"), model=BH_Bayes_fn)

summary(BH_Bayes_fit)

boxplot(unname(as.matrix(BH_Bayes_fit)), range=0, border="darkgrey")
lines(c(NA, Y))
```

If we want to obtain predictions that are invariant to parameterization and correct in the frequentist sense, we need to modify this approach slightly. We need to change the prior distribution to the asymptotic distribution of the MLE. This can be done quite easily as follows. 

```{r}
BH_DC_pred_fn= function() {
  # Likelihood
  X[1] ~ dnorm(mu0, 1 / sigma^2) # Initial condition
  N[1] <- exp(X[1])
  for(i in 2:(n+1)){
    Y[(i-1)] ~ dpois(exp(X[i])) 
    X[i] ~ dnorm(mu[i], 1 / sigma^2) 
    mu[i] <- X[(i-1)] + log(lambda) - log(1 + beta * exp(X[(i-1)]))
    N[i] <- exp(X[i])
  }

  # Priors on model parameters: they are on the real line.
  parms ~ dmnorm(MuPost,PrecPost)
  ln.beta <- parms[1] 
  ln.sigma <- parms[2]
  ln.tmp <- parms[3]

  # Parameters on the natural scale
  beta <- exp(ln.beta)
  sigma <- exp(ln.sigma)
  tmp <- exp(ln.tmp)
  lambda <- tmp + 1
  mu0 <- log(2)  + log(lambda) - log(1 + beta * 2)
}

# Gause's data 
Y = Paramecium
dat = list(n=length(Y), Y=Y, MuPost=coef(BH_MLE), PrecPost=solve(vcov(BH_MLE)))
BH_DC_Pred = jags.fit(data=dat, params=c("N"), model=BH_DC_pred_fn)

summary(BH_DC_Pred)

boxplot(unname(as.matrix(BH_DC_Pred)), range=0, border="darkgrey")
lines(c(NA, Y))
```

If we want to forecast future observations, we modify the program slightly. To do this, we pretend as if the process had run longer than $T_{max}$ but we could not 'observe' those future states. Thus, it becomes a missing data problem. Let us see how we can do this.

```{r}
BH_Bayes_fn= function() {
  X[1] ~ dnorm(mu0, 1 / sigma^2) # Initial condition
  N[1] <- exp(X[1])
  for(i in 2:(n+1)){
    Y[(i-1)] ~ dpois(exp(X[i]))
  }
  for (i in 2:N_future){
    X[i] ~ dnorm(mu[i], 1 / sigma^2) 
    mu[i] <- X[(i-1)] + log(lambda) - log(1 + beta * exp(X[(i-1)]))
    N[i] <- exp(X[i])
  }

  # Priors on model parameters: they are on the real line.
  ln.beta ~ dnorm(0, 0.1) 
  ln.sigma ~ dnorm(0, 0.1)
  ln.tmp ~ dnorm(0, 0.1)

  # Parameters on the natural scale
  beta <- exp(ln.beta)
  sigma <- exp(ln.sigma)
  tmp <- exp(ln.tmp)
  lambda <- tmp + 1
  mu0 <- log(2)  + log(lambda) - log(1 + beta * 2)
}

# Gause's data 
Y = Paramecium

# We want to predict 3 years in future.
dat = list(n=length(Y), Y=Y, N_future=length(Y)+3)

BH_Bayes_fit = jags.fit(data=dat, params=c("N[19:21]"), model=BH_Bayes_fn)

summary(BH_Bayes_fit)

pred = mcmcapply(BH_Bayes_fit, quantile, c(0.05, 0.5, 0.95))
plot(c(Y, NA, NA, NA), type="l", ylim=c(0, max(Y, pred)), xlab="Time", ylab="Value")
matlines(c(18:21), 
    t(cbind(rep(Y[length(Y)], 3), pred)),
    col=4, lty=c(2,1,2))
```

To get the frequentist version, we modify this prediction function similarly. The prior is equal to the asymptotic distribution of the MLE. We will leave it to you to try that out.

## Other considerations

### What is going on inside jags.fit

```{r}
library(dclone)

n = 30
X1 = rnorm(n)
X = model.matrix(~X1)
beta.true = c(0.5, 1)
link_mu = X %*% beta.true

# Linear regression model
mu = link_mu
sigma.e = 1
Y = rnorm(n,mean=mu,sd=sigma.e)

Normal.model = function(){
  # Likelihood 
  for (i in 1:n){
    mu[i] <- X[i,] %*% beta
    Y[i] ~ dnorm(mu[i],prec.e)
  }
  # Prior
  beta[1] ~ dnorm(0, 1)
  beta[2] ~ dnorm(0, 1)
  prec.e ~ dlnorm(0, 1)
}

dat = list(Y=Y, X=X, n=n)

Normal.Bayes = jags.fit(data=dat, params=c("beta","prec.e"), model=Normal.model)
```

What just happened? `jags.fit` is a wrapper around some rjags functions.

```{r}
m <- jagsModel(file = Normal.model, data=dat, n.chains=3)
m
str(m)
str(m$state())
m$iter()

update(m, n.iter=1000)
str(m$state())
m$iter()

s = codaSamples(m, variable.names=c("beta","prec.e"), n.iter=5000)
str(s)
head(s)
m$iter()

update(m, n.iter=1000)
m$iter()

s = codaSamples(m, variable.names=c("beta","prec.e"), n.iter=5000)
head(s)
m$iter()
```

Can we further update `Normal.Bayes`?

```{r}
m2 <- updated.model(Normal.Bayes)
m2$iter()
update(m2, n.iter=1000)
m2$iter()
```

Couple of things to cover here:

```{r}
str(formals(jags.fit))
```

Implicitly, we do:

```{r}
Normal.Bayes = jags.fit(
    data=dat, 
    params=c("beta","prec.e"),
    model=Normal.model,
    inits=NULL,
    n.chains=3,
    n.adapt=1000,
    n.update=1000,
    thin=1,
    n.iter=5000,
    updated.model=TRUE)
```

### Working with MCMC lists

```{r}
mcmcapply(Normal.Bayes, sd)

mcmcapply(Normal.Bayes, quantile, c(0.05, 0.5, 0.95))
quantile(Normal.Bayes, c(0.05, 0.5, 0.95))

str(as.matrix(Normal.Bayes))
```

### We need to talk about RNGs

```{r}
library(rjags)
str(parallel.seeds("base::BaseRNG", 5))

## The lecuyer module provides the RngStream factory, which allows large
## numbers of independent parallel RNGs to be generated. 
load.module("lecuyer")
list.factories(type="rng")
str(parallel.seeds("lecuyer::RngStream", 5))
```

### Can we run chains in parallel?

```{r}
system.time({
    Normal.Bayes = jags.fit(
        data=dat, params=c("beta","prec.e"), model=Normal.model,
        n.chains=4,
        n.update=10^6)
})

if (.Platform$OS.type != "windows") {
system.time({
    Normal.Bayes = jags.parfit(
        cl=10,
        data=dat, params=c("beta","prec.e"), model=Normal.model,
        n.chains=4,
        n.update=10^6)
})
}
```

### I like DC but I don't have time ...

Well, check `dc.parfit`

```{r}
## determine the number of workers needed
clusterSize(1:5)

## visually compare balancing options
opar <- par(mfrow=c(2, 2))
plotClusterSize(2,1:5, "none")
plotClusterSize(2,1:5, "load")
plotClusterSize(2,1:5, "size")
plotClusterSize(2,1:5, "both")
par(opar)
```

Parallel chains, size balancing, both.

## References

Lele, S.R., B. Dennis and F. Lutscher, 2007. Data cloning: easy maximum likelihood estimation for complex ecological models using Bayesian Markov chain Monte Carlo methods. Ecology Letters, 10:551-563.

Lele, S. R., and Dennis, B., 2009. Bayesian methods for hierarchical models: are ecologists making a Faustian bargain? Ecological Applications, 19:581-584.

Ponciano, J. M., Taper, M. L., Dennis, B., and Lele, S. R., 2009. Hierarchical models in ecology: confidence intervals, hypothesis testing, and model selection using data cloning. Ecology, 90:356-362.

Lele, S. R., Nadeem, K., and Schmuland, B., 2010. Estimability and likelihood inference for generalized linear mixed models using data cloning. Journal of the American Statistical Association, 105:1617-1625.

Lele, S. R., 2010. Model complexity and information in the data: Could it be a house built on sand? Ecology, 91(12):3493-3496.

Slymos, P., 2010. dclone: Data Cloning in R. The R Journal, 2(2):29-37.

Lele, S. R., 2020. Consequences of lack of parameterization invariance of non-informative bayesian analysis for wildlife management: survival of San Joaquin Kit Fox and declines in amphibian populations. Front. Ecol. Evol., 7:501.

Lele, S. R., 2020. How should we quantify uncertainty in statistical inference? Front. Ecol. Evol., 8:35.
